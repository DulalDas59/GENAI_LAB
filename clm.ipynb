{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A concrete NeMo GPT CLM training command (from NVIDIA docs)\n",
    "\n",
    "NeMo provides megatron_gpt_pretraining.py with Hydra config overrides (devices, steps, batch sizes, TP/PP, optimizer schedule, etc.). Example from the NeMo GPT training guide:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "python <NeMo_ROOT>/examples/nlp/language_modeling/megatron_gpt_pretraining.py \\\n",
    "  --config-path=<NeMo_ROOT>/examples/nlp/language_modeling/conf \\\n",
    "  --config-name=megatron_gpt_config \\\n",
    "  trainer.devices=1 trainer.num_nodes=1 trainer.max_steps=300000 trainer.precision=16 \\\n",
    "  model.micro_batch_size=6 model.global_batch_size=192 \\\n",
    "  model.tensor_model_parallel_size=1 model.pipeline_model_parallel_size=1 \\\n",
    "  model.encoder_seq_length=1024 model.max_position_embeddings=1024 \\\n",
    "  model.optim.name=fused_adam model.optim.lr=6e-4 model.optim.sched.name=CosineAnnealing\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Minimal “gold standard” PyTorch CLM step (so you truly understand it)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def clm_loss(logits, input_ids, pad_id=None):\n",
    "    \"\"\"\n",
    "    logits: [B, T, V]\n",
    "    input_ids: [B, T]\n",
    "    \"\"\"\n",
    "    # shift: predict token t+1 from positions <= t\n",
    "    shift_logits = logits[:, :-1, :].contiguous()     # [B, T-1, V]\n",
    "    shift_labels = input_ids[:, 1:].contiguous()      # [B, T-1]\n",
    "\n",
    "    if pad_id is None:\n",
    "        return F.cross_entropy(shift_logits.view(-1, shift_logits.size(-1)),\n",
    "                               shift_labels.view(-1))\n",
    "    else:\n",
    "        return F.cross_entropy(shift_logits.view(-1, shift_logits.size(-1)),\n",
    "                               shift_labels.view(-1),\n",
    "                               ignore_index=pad_id)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is how you configure a Causal Language Modeling run for Continued Pre-training on enterprise data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import (\n",
    "    AutoModelForCausalLM, \n",
    "    AutoTokenizer, \n",
    "    TrainingArguments, \n",
    "    Trainer,\n",
    "    DataCollatorForLanguageModeling\n",
    ")\n",
    "import torch\n",
    "\n",
    "# 1. Load Pre-trained Model (e.g., Llama-3-8B)\n",
    "model_id = \"meta-llama/Meta-Llama-3-8B\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "\n",
    "# Enterprise Note: Llama doesn't have a default pad token, usually set to EOS\n",
    "tokenizer.pad_token = tokenizer.eos_token \n",
    "\n",
    "# 2. Dataset Preparation (Concept)\n",
    "# Assume 'tokenized_datasets' contains your internal domain data\n",
    "# packed into blocks of block_size=2048 or 4096.\n",
    "\n",
    "# 3. Data Collator\n",
    "# This handles the dynamic masking (if needed) but for CLM it just shifts labels\n",
    "data_collator = DataCollatorForLanguageModeling(\n",
    "    tokenizer=tokenizer, \n",
    "    mlm=False  # Crucial: False for Causal LM, True for BERT\n",
    ")\n",
    "\n",
    "# 4. Enterprise Training Arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./cpt-llama-finance\",\n",
    "    \n",
    "    # 3D Parallelism / Efficiency Params\n",
    "    per_device_train_batch_size=4,\n",
    "    gradient_accumulation_steps=4, # Simulates larger batch size\n",
    "    fp16=False,\n",
    "    bf16=True, # Brain Float 16 is MANDATORY for Llama stability\n",
    "    \n",
    "    # Optimizer\n",
    "    learning_rate=2e-5, # Low LR for continued pre-training\n",
    "    weight_decay=0.01,\n",
    "    lr_scheduler_type=\"cosine\", # Standard for LLMs\n",
    "    warmup_ratio=0.03, # Prevents loss spikes at start\n",
    "    \n",
    "    # Distributed Training Strategy (DeepSpeed/FSDP)\n",
    "    # In production, you would pass a deepspeed config file here\n",
    "    # deepspeed=\"ds_config_zero3.json\", \n",
    "    \n",
    "    logging_steps=10,\n",
    "    save_strategy=\"epoch\",\n",
    ")\n",
    "\n",
    "# 5. Initialize Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_datasets[\"train\"],\n",
    "    eval_dataset=tokenized_datasets[\"validation\"],\n",
    "    data_collator=data_collator,\n",
    ")\n",
    "\n",
    "# 6. Train\n",
    "# trainer.train()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
