{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>(A) Hugging Face generate() with advanced sampling knobs</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/home/jovyan/.venv/lib/python3.11/site-packages/huggingface_hub/file_download.py:942: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Using pad_token, but it is not set yet.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Explain self-attention in one paragraph.\n",
      "Acknowledge the importance of reading and writing while staying true to your original purpose, such as understanding how things work (which will help you keep track of what is important) or keeping an eye on yourself when it comes time for a \"drink.\" This can be helpful if there are situations where something seems out of place: For example, some people don't want to talk about their weight problems because they feel that's upsetting them; but others just think this may have been done by someone else who wants answers from other folks without having read through all his/her thoughts first before giving him\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, set_seed\n",
    "\n",
    "model_id = \"gpt2\"  # swap to your local Llama/Mistral checkpoint if available\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(model_id).eval()\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model.to(device)\n",
    "\n",
    "prompt = \"Explain self-attention in one paragraph.\"\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "\n",
    "set_seed(42)\n",
    "\n",
    "gen = model.generate(\n",
    "    **inputs,\n",
    "    max_new_tokens=120,\n",
    "\n",
    "    # ----- sampling must be ON -----\n",
    "    do_sample=True,\n",
    "\n",
    "    # ----- core sampling -----\n",
    "    temperature=0.8,      # <1 = more deterministic\n",
    "    top_p=0.9,            # nucleus\n",
    "    top_k=50,             # top-k (often combined with top_p)\n",
    "    typical_p=0.95,       # locally typical sampling (optional)\n",
    "\n",
    "    # ----- anti-repetition -----\n",
    "    repetition_penalty=1.1,\n",
    "    no_repeat_ngram_size=3,\n",
    ")\n",
    "\n",
    "print(tokenizer.decode(gen[0], skip_special_tokens=True))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>(B) Implement temperature + top-k + top-p from scratch (single step)</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def sample_next_token(logits, temperature=1.0, top_k=None, top_p=None):\n",
    "    \"\"\"\n",
    "    logits: (V,) tensor for next token\n",
    "    returns: sampled token_id (int)\n",
    "    \"\"\"\n",
    "    # 1) temperature\n",
    "    if temperature is not None and temperature > 0:\n",
    "        logits = logits / temperature\n",
    "\n",
    "    # 2) top-k filter\n",
    "    if top_k is not None and top_k > 0:\n",
    "        v, _ = torch.topk(logits, top_k)\n",
    "        min_keep = v[-1]\n",
    "        logits = torch.where(logits < min_keep, torch.tensor(float(\"-inf\"), device=logits.device), logits)\n",
    "\n",
    "    # 3) top-p (nucleus) filter\n",
    "    if top_p is not None and 0 < top_p < 1:\n",
    "        probs = torch.softmax(logits, dim=-1)\n",
    "        sorted_probs, sorted_idx = torch.sort(probs, descending=True)\n",
    "        cum = torch.cumsum(sorted_probs, dim=-1)\n",
    "\n",
    "        # keep smallest set with cum prob >= top_p\n",
    "        keep = cum <= top_p\n",
    "        keep[..., 0] = True  # always keep at least 1\n",
    "        filtered_idx = sorted_idx[keep]\n",
    "        mask = torch.ones_like(logits, dtype=torch.bool)\n",
    "        mask[filtered_idx] = False\n",
    "        logits = logits.masked_fill(mask, float(\"-inf\"))\n",
    "\n",
    "    # 4) sample\n",
    "    probs = torch.softmax(logits, dim=-1)\n",
    "    return int(torch.multinomial(probs, num_samples=1).item())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Here is a PyTorch implementation of Temperature + Top-K + Top-P.</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Greedy: 0\n",
      "Sampled Index: 0\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def sample_next_token(logits, temperature=1.0, top_k=0, top_p=0.0):\n",
    "    \"\"\"\n",
    "    logits: [batch_size, vocab_size] - Raw output from the model's last layer\n",
    "    \"\"\"\n",
    "    \n",
    "    # 1. Apply Temperature\n",
    "    # Higher T makes distribution flatter (more random)\n",
    "    # Lower T makes distribution sharper (more deterministic)\n",
    "    if temperature != 1.0:\n",
    "        logits = logits / temperature\n",
    "\n",
    "    # 2. Filter: Top-K Sampling\n",
    "    # Keep only the top K tokens, mask the rest to -infinity\n",
    "    if top_k > 0:\n",
    "        # Find the value of the K-th sorted element\n",
    "        top_k_values, _ = torch.topk(logits, top_k)\n",
    "        # The cutoff is the smallest value in the top K\n",
    "        k_cutoff = top_k_values[:, -1].unsqueeze(1)\n",
    "        # Mask everything below cutoff\n",
    "        logits = torch.where(logits < k_cutoff, torch.tensor(float('-inf')), logits)\n",
    "\n",
    "    # 3. Filter: Top-P (Nucleus) Sampling\n",
    "    # Keep the top tokens with cumulative probability >= top_p\n",
    "    if top_p > 0.0:\n",
    "        # Sort logits in descending order\n",
    "        sorted_logits, sorted_indices = torch.sort(logits, descending=True)\n",
    "        # Convert to probabilities\n",
    "        cumulative_probs = torch.cumsum(F.softmax(sorted_logits, dim=-1), dim=-1)\n",
    "\n",
    "        # Create a mask for tokens to REMOVE\n",
    "        # We shift the mask right by 1 to always keep at least the first token\n",
    "        sorted_indices_to_remove = cumulative_probs > top_p\n",
    "        sorted_indices_to_remove[..., 1:] = sorted_indices_to_remove[..., :-1].clone()\n",
    "        sorted_indices_to_remove[..., 0] = 0\n",
    "\n",
    "        # Scatter the mask back to the original indices\n",
    "        indices_to_remove = sorted_indices_to_remove.scatter(1, sorted_indices, sorted_indices_to_remove)\n",
    "        logits = logits.masked_fill(indices_to_remove, float('-inf'))\n",
    "\n",
    "    # 4. Final Sampling\n",
    "    # Convert filtered logits to probabilities\n",
    "    probs = F.softmax(logits, dim=-1)\n",
    "    \n",
    "    # Sample from the distribution\n",
    "    next_token = torch.multinomial(probs, num_samples=1)\n",
    "    \n",
    "    return next_token\n",
    "\n",
    "# --- Example Usage ---\n",
    "# Fake logits for a vocab of 10 words\n",
    "fake_logits = torch.tensor([[10.0, 9.0, 8.0, 2.0, 1.0, 1.0, 1.0, 0.5, 0.5, 0.1]])\n",
    "\n",
    "# Scenario A: Greedy (Effectively Temp -> 0)\n",
    "print(f\"Greedy: {torch.argmax(fake_logits)}\") \n",
    "\n",
    "# Scenario B: High Temp + Nucleus (Creative)\n",
    "token = sample_next_token(fake_logits, temperature=1.2, top_p=0.9)\n",
    "print(f\"Sampled Index: {token.item()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'trtllm' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Pseudo-code for TensorRT-LLM Sampling Configuration\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m sampling_config = \u001b[43mtrtllm\u001b[49m.runtime.SamplingConfig(\n\u001b[32m      3\u001b[39m     temperature=\u001b[32m0.7\u001b[39m,       \u001b[38;5;66;03m# Balance creativity/coherence\u001b[39;00m\n\u001b[32m      4\u001b[39m     top_k=\u001b[32m0\u001b[39m,               \u001b[38;5;66;03m# Disable Top-K (rely on Top-P)\u001b[39;00m\n\u001b[32m      5\u001b[39m     top_p=\u001b[32m0.95\u001b[39m,            \u001b[38;5;66;03m# Nucleus Sampling\u001b[39;00m\n\u001b[32m      6\u001b[39m     repetition_penalty=\u001b[32m1.2\u001b[39m \u001b[38;5;66;03m# Critical: Penalize tokens that appeared recently\u001b[39;00m\n\u001b[32m      7\u001b[39m )\n",
      "\u001b[31mNameError\u001b[39m: name 'trtllm' is not defined"
     ]
    }
   ],
   "source": [
    "# Pseudo-code for TensorRT-LLM Sampling Configuration\n",
    "sampling_config = trtllm.runtime.SamplingConfig(\n",
    "    temperature=0.7,       # Balance creativity/coherence\n",
    "    top_k=0,               # Disable Top-K (rely on Top-P)\n",
    "    top_p=0.95,            # Nucleus Sampling\n",
    "    repetition_penalty=1.2 # Critical: Penalize tokens that appeared recently\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
