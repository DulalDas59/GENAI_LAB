{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>1) Minimal working example (HuggingFace): draft + target (speculative decoding)\n",
    "\n",
    "Concept: a small assistant/draft model proposes multiple tokens; the big target model verifies them in fewer expensive forward passes → higher tokens/sec. This is the classic speculative decoding idea (Leviathan et al.).</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>A) Greedy speculative decoding (deterministic)</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# Example pair from HF docs (swap to your target/draft pair that share tokenizer)\n",
    "target_id = \"HuggingFaceTB/SmolLM-1.7B\"\n",
    "draft_id  = \"HuggingFaceTB/SmolLM-135M\"\n",
    "\n",
    "tok = AutoTokenizer.from_pretrained(target_id)\n",
    "target = AutoModelForCausalLM.from_pretrained(target_id, dtype=\"auto\").to(device).eval()\n",
    "draft  = AutoModelForCausalLM.from_pretrained(draft_id,  dtype=\"auto\").to(device).eval()\n",
    "\n",
    "prompt = \"Explain speculative decoding in one paragraph for LLM serving.\"\n",
    "inputs = tok(prompt, return_tensors=\"pt\").to(device)\n",
    "\n",
    "with torch.inference_mode():\n",
    "    t0 = time.perf_counter()\n",
    "    out = target.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=200,\n",
    "        do_sample=False,                 # greedy\n",
    "        assistant_model=draft            # <-- speculative decoding trigger\n",
    "    )\n",
    "    t1 = time.perf_counter()\n",
    "\n",
    "text = tok.decode(out[0], skip_special_tokens=True)\n",
    "gen_tokens = out.shape[1] - inputs[\"input_ids\"].shape[1]\n",
    "tok_per_s = gen_tokens / (t1 - t0)\n",
    "\n",
    "print(text)\n",
    "print(f\"\\nGenerated tokens: {gen_tokens}, time: {t1-t0:.3f}s, tokens/sec: {tok_per_s:.2f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "HF supports speculative decoding via assistant_model and notes it works for greedy + sampling (and has some limitations like batching)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>B) Speculative sampling (stochastic)</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When you want “sampling behavior” (top-p, temperature), set do_sample=True. HF will resample on validation failures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.inference_mode():\n",
    "    t0 = time.perf_counter()\n",
    "    out = target.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=200,\n",
    "        do_sample=True,\n",
    "        temperature=0.7,\n",
    "        top_p=0.9,\n",
    "        assistant_model=draft\n",
    "    )\n",
    "    t1 = time.perf_counter()\n",
    "\n",
    "gen_tokens = out.shape[1] - inputs[\"input_ids\"].shape[1]\n",
    "print(f\"tokens/sec: {gen_tokens/(t1-t0):.2f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
