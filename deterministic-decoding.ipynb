{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>1) Enterprise code: Greedy + Beam (HuggingFace Transformers, decoder-only CausalLM)\n",
    "What matters for certification\n",
    "\n",
    "Greedy: num_beams=1, do_sample=False → fastest, deterministic, but can be repetitive / myopic.\n",
    "\n",
    "Beam search: num_beams>1, do_sample=False → higher likelihood sequences, great for translation/summarization; can become generic/long unless tuned (length_penalty, early_stopping).</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from dataclasses import dataclass\n",
    "from typing import List, Optional, Literal, Dict, Any\n",
    "\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "\n",
    "DecodeMode = Literal[\"greedy\", \"beam\"]\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class DecodeConfig:\n",
    "    max_new_tokens: int = 128\n",
    "    min_new_tokens: int = 0\n",
    "    temperature: float = 1.0  # ignored when do_sample=False, kept for completeness\n",
    "    top_p: float = 1.0\n",
    "    top_k: int = 0\n",
    "\n",
    "    # Beam-search knobs\n",
    "    num_beams: int = 4\n",
    "    length_penalty: float = 1.0\n",
    "    early_stopping: bool = True\n",
    "    num_return_sequences: int = 1\n",
    "\n",
    "    # Safety / quality knobs\n",
    "    repetition_penalty: float = 1.0\n",
    "    no_repeat_ngram_size: int = 0\n",
    "\n",
    "    # Runtime\n",
    "    use_fp16: bool = True\n",
    "\n",
    "\n",
    "class TextGenerator:\n",
    "    \"\"\"\n",
    "    Enterprise-ish wrapper:\n",
    "    - Handles tokenizer padding\n",
    "    - Batches prompts\n",
    "    - Measures latency + tokens/sec\n",
    "    - Supports greedy and beam search deterministically\n",
    "    \"\"\"\n",
    "    def __init__(self, model_name: str, device: Optional[str] = None):\n",
    "        self.device = device or (\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=True)\n",
    "        self.model = AutoModelForCausalLM.from_pretrained(\n",
    "            model_name,\n",
    "            torch_dtype=torch.float16 if (self.device == \"cuda\") else None,\n",
    "            device_map=\"auto\" if self.device == \"cuda\" else None,\n",
    "        )\n",
    "\n",
    "        # Many decoder-only LMs (e.g., Llama) may not define a pad token by default.\n",
    "        if self.tokenizer.pad_token_id is None:\n",
    "            self.tokenizer.pad_token = self.tokenizer.eos_token\n",
    "\n",
    "        self.model.eval()\n",
    "\n",
    "    @torch.inference_mode()\n",
    "    def generate(\n",
    "        self,\n",
    "        prompts: List[str],\n",
    "        mode: DecodeMode,\n",
    "        cfg: Optional[DecodeConfig] = None,\n",
    "    ) -> Dict[str, Any]:\n",
    "        cfg = cfg or DecodeConfig()\n",
    "\n",
    "        enc = self.tokenizer(\n",
    "            prompts,\n",
    "            return_tensors=\"pt\",\n",
    "            padding=True,\n",
    "            truncation=True,\n",
    "        )\n",
    "        input_ids = enc[\"input_ids\"].to(self.device)\n",
    "        attention_mask = enc[\"attention_mask\"].to(self.device)\n",
    "\n",
    "        # ---- Greedy vs Beam configuration ----\n",
    "        if mode == \"greedy\":\n",
    "            gen_kwargs = dict(\n",
    "                do_sample=False,\n",
    "                num_beams=1,\n",
    "                num_return_sequences=1,\n",
    "            )\n",
    "        elif mode == \"beam\":\n",
    "            gen_kwargs = dict(\n",
    "                do_sample=False,\n",
    "                num_beams=cfg.num_beams,\n",
    "                num_return_sequences=cfg.num_return_sequences,\n",
    "                length_penalty=cfg.length_penalty,\n",
    "                early_stopping=cfg.early_stopping,\n",
    "            )\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown mode: {mode}\")\n",
    "\n",
    "        # Common generation knobs (work for both greedy & beam)\n",
    "        gen_kwargs.update(\n",
    "            max_new_tokens=cfg.max_new_tokens,\n",
    "            min_new_tokens=cfg.min_new_tokens,\n",
    "            repetition_penalty=cfg.repetition_penalty,\n",
    "            no_repeat_ngram_size=cfg.no_repeat_ngram_size,\n",
    "            pad_token_id=self.tokenizer.pad_token_id,\n",
    "            eos_token_id=self.tokenizer.eos_token_id,\n",
    "            return_dict_in_generate=True,\n",
    "            output_scores=False,\n",
    "        )\n",
    "\n",
    "        # Optional fp16 autocast (CUDA only)\n",
    "        start = time.perf_counter()\n",
    "        if self.device == \"cuda\" and cfg.use_fp16:\n",
    "            with torch.autocast(device_type=\"cuda\", dtype=torch.float16):\n",
    "                out = self.model.generate(\n",
    "                    input_ids=input_ids,\n",
    "                    attention_mask=attention_mask,\n",
    "                    **gen_kwargs,\n",
    "                )\n",
    "        else:\n",
    "            out = self.model.generate(\n",
    "                input_ids=input_ids,\n",
    "                attention_mask=attention_mask,\n",
    "                **gen_kwargs,\n",
    "            )\n",
    "        end = time.perf_counter()\n",
    "\n",
    "        # Decode\n",
    "        sequences = out.sequences\n",
    "        texts = self.tokenizer.batch_decode(sequences, skip_special_tokens=True)\n",
    "\n",
    "        # Basic throughput estimate\n",
    "        total_new_tokens = sequences.shape[1] - input_ids.shape[1]\n",
    "        latency_s = end - start\n",
    "        tok_per_s = (total_new_tokens * sequences.shape[0]) / max(latency_s, 1e-9)\n",
    "\n",
    "        return {\n",
    "            \"mode\": mode,\n",
    "            \"texts\": texts,\n",
    "            \"latency_s\": latency_s,\n",
    "            \"tokens_per_s_est\": tok_per_s,\n",
    "            \"batch_size\": len(prompts),\n",
    "        }\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Use any CausalLM you have access to locally.\n",
    "    # Example: \"gpt2\", or a local Llama checkpoint path.\n",
    "    model_name = \"gpt2\"\n",
    "\n",
    "    tg = TextGenerator(model_name)\n",
    "\n",
    "    prompts = [\n",
    "        \"Write a concise explanation of beam search in LLM decoding.\",\n",
    "        \"Give 3 bullet points on when greedy decoding is sufficient.\",\n",
    "    ]\n",
    "\n",
    "    greedy = tg.generate(prompts, mode=\"greedy\", cfg=DecodeConfig(max_new_tokens=80))\n",
    "    print(\"\\n=== GREEDY ===\")\n",
    "    print(\"latency_s:\", greedy[\"latency_s\"], \"tok/s:\", greedy[\"tokens_per_s_est\"])\n",
    "    print(greedy[\"texts\"][0], \"\\n---\\n\", greedy[\"texts\"][1])\n",
    "\n",
    "    beam = tg.generate(prompts, mode=\"beam\", cfg=DecodeConfig(max_new_tokens=80, num_beams=4, length_penalty=0.8))\n",
    "    print(\"\\n=== BEAM ===\")\n",
    "    print(\"latency_s:\", beam[\"latency_s\"], \"tok/s:\", beam[\"tokens_per_s_est\"])\n",
    "    print(beam[\"texts\"][0], \"\\n---\\n\", beam[\"texts\"][1])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>2) Enterprise code: Greedy + Beam in Triton + TensorRT-LLM (NVIDIA production stack)\n",
    "What matters for certification\n",
    "\n",
    "In TensorRT-LLM backend, beam search must be supported by:\n",
    "\n",
    "Engine build time: set --max_beam_width > 1 for encoder-decoder builds (and similarly for decoder engines). \n",
    "NVIDIA Docs\n",
    "\n",
    "Triton model config: max_beam_width and decoding_mode. Default behavior: if max_beam_width == 1, it defaults to top-k/top-p mode; otherwise defaults to beam search. \n",
    "NVIDIA Docs\n",
    "\n",
    "At request time, you set input tensor beam_width:\n",
    "\n",
    "beam_width = 1 → greedy\n",
    "\n",
    "beam_width > 1 → beam search</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tritonclient.http as httpclient\n",
    "\n",
    "\n",
    "def np_scalar(value, dtype):\n",
    "    return np.array([value], dtype=dtype)\n",
    "\n",
    "\n",
    "def infer_trtllm(\n",
    "    triton_url: str,\n",
    "    model_name: str,\n",
    "    input_ids: np.ndarray,          # shape: [B, S], dtype=int32\n",
    "    input_lengths: np.ndarray,      # shape: [B, 1], dtype=int32\n",
    "    request_output_len: int,\n",
    "    beam_width: int = 1,            # 1 => greedy, >1 => beam search\n",
    "    repetition_penalty: float = 1.0,\n",
    "    presence_penalty: float = 0.0,\n",
    "    frequency_penalty: float = 0.0,\n",
    "):\n",
    "    \"\"\"\n",
    "    Enterprise-ish skeleton for TRT-LLM Triton backend.\n",
    "    You typically front this with preprocessing/postprocessing models,\n",
    "    but the core decoding control (beam_width, penalties) is here.\n",
    "    \"\"\"\n",
    "    client = httpclient.InferenceServerClient(url=triton_url, verbose=False)\n",
    "\n",
    "    inputs = []\n",
    "\n",
    "    # Core token inputs (often produced by the 'preprocessing' model in the ensemble).\n",
    "    inp_input_ids = httpclient.InferInput(\"input_ids\", input_ids.shape, \"INT32\")\n",
    "    inp_input_ids.set_data_from_numpy(input_ids)\n",
    "    inputs.append(inp_input_ids)\n",
    "\n",
    "    inp_input_lengths = httpclient.InferInput(\"input_lengths\", input_lengths.shape, \"INT32\")\n",
    "    inp_input_lengths.set_data_from_numpy(input_lengths)\n",
    "    inputs.append(inp_input_lengths)\n",
    "\n",
    "    # Decoding controls (documented common inputs)\n",
    "    inp_beam = httpclient.InferInput(\"beam_width\", [1], \"INT32\")\n",
    "    inp_beam.set_data_from_numpy(np_scalar(beam_width, np.int32))\n",
    "    inputs.append(inp_beam)\n",
    "\n",
    "    inp_rep = httpclient.InferInput(\"repetition_penalty\", [1], \"FP32\")\n",
    "    inp_rep.set_data_from_numpy(np_scalar(repetition_penalty, np.float32))\n",
    "    inputs.append(inp_rep)\n",
    "\n",
    "    inp_pres = httpclient.InferInput(\"presence_penalty\", [1], \"FP32\")\n",
    "    inp_pres.set_data_from_numpy(np_scalar(presence_penalty, np.float32))\n",
    "    inputs.append(inp_pres)\n",
    "\n",
    "    inp_freq = httpclient.InferInput(\"frequency_penalty\", [1], \"FP32\")\n",
    "    inp_freq.set_data_from_numpy(np_scalar(frequency_penalty, np.float32))\n",
    "    inputs.append(inp_freq)\n",
    "\n",
    "    # Typical control tensors (names can vary by template/client; adapt to your deployed model)\n",
    "    out_len = httpclient.InferInput(\"request_output_len\", [1], \"INT32\")\n",
    "    out_len.set_data_from_numpy(np_scalar(request_output_len, np.int32))\n",
    "    inputs.append(out_len)\n",
    "\n",
    "    # What outputs you ask for depends on your backend template\n",
    "    outputs = [\n",
    "        httpclient.InferRequestedOutput(\"output_ids\"),\n",
    "        httpclient.InferRequestedOutput(\"sequence_length\"),\n",
    "    ]\n",
    "\n",
    "    resp = client.infer(model_name=model_name, inputs=inputs, outputs=outputs)\n",
    "    output_ids = resp.as_numpy(\"output_ids\")\n",
    "    seq_lens = resp.as_numpy(\"sequence_length\")\n",
    "    return output_ids, seq_lens\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Example shapes (B=1, S=4). In practice, get these from your tokenizer/preprocessing model.\n",
    "    input_ids = np.array([[101, 102, 103, 104]], dtype=np.int32)\n",
    "    input_lengths = np.array([[4]], dtype=np.int32)\n",
    "\n",
    "    # Greedy\n",
    "    out_ids_g, out_len_g = infer_trtllm(\n",
    "        triton_url=\"localhost:8000\",\n",
    "        model_name=\"tensorrt_llm\",\n",
    "        input_ids=input_ids,\n",
    "        input_lengths=input_lengths,\n",
    "        request_output_len=64,\n",
    "        beam_width=1,  # greedy\n",
    "    )\n",
    "    print(\"GREEDY output_ids:\", out_ids_g, \"lens:\", out_len_g)\n",
    "\n",
    "    # Beam\n",
    "    out_ids_b, out_len_b = infer_trtllm(\n",
    "        triton_url=\"localhost:8000\",\n",
    "        model_name=\"tensorrt_llm\",\n",
    "        input_ids=input_ids,\n",
    "        input_lengths=input_lengths,\n",
    "        request_output_len=64,\n",
    "        beam_width=4,  # beam search\n",
    "        repetition_penalty=1.05,\n",
    "    )\n",
    "    print(\"BEAM output_ids:\", out_ids_b, \"lens:\", out_len_b)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>A. Greedy Decoding (Fast, Low Memory)\n",
    "Logic: Always pick the highest probability token.\n",
    "\n",
    "Memory: 1 Sequence per User.</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def greedy_decoding(model, tokenizer, prompt, max_new_tokens=50):\n",
    "    input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids.to(\"cuda\")\n",
    "    \n",
    "    # KV Cache setup (Simulated for clarity - real implementations verify cache shape)\n",
    "    past_key_values = None\n",
    "\n",
    "    for _ in range(max_new_tokens):\n",
    "        with torch.no_grad():\n",
    "            # 1. Forward pass (only pass the last token if using KV cache)\n",
    "            outputs = model(input_ids, past_key_values=past_key_values)\n",
    "            \n",
    "            # 2. Get Logits for the last token\n",
    "            next_token_logits = outputs.logits[:, -1, :]\n",
    "            \n",
    "            # 3. Greedy Selection (Argmax)\n",
    "            next_token_id = torch.argmax(next_token_logits, dim=-1).unsqueeze(-1)\n",
    "            \n",
    "            # 4. Append to sequence\n",
    "            input_ids = torch.cat([input_ids, next_token_id], dim=-1)\n",
    "            \n",
    "            # 5. Update KV Cache for next step\n",
    "            past_key_values = outputs.past_key_values\n",
    "            \n",
    "            # 6. Stop if EOS is generated\n",
    "            if next_token_id.item() == tokenizer.eos_token_id:\n",
    "                break\n",
    "                \n",
    "    return tokenizer.decode(input_ids[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>B. Beam Search (High Quality, High Memory)Logic: Maintain top $K$ sequences.Memory: $K$ Sequences per User (High VRAM usage).Exam Key: Notice the Score Calculation with length_penalty.</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "def beam_search_step(model, input_ids, beam_width=4, length_penalty=1.0):\n",
    "    # This is a simplified single-step visualization of the Beam Search Logic\n",
    "    \n",
    "    # Assume we have 'beam_width' number of candidate sequences from previous step\n",
    "    # Shape: [beam_width, current_seq_len]\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_ids)\n",
    "        next_token_logits = outputs.logits[:, -1, :] # [beam_width, vocab_size]\n",
    "        \n",
    "        # 1. Convert to Log Probabilities (Scores are additive in log-space)\n",
    "        next_token_probs = F.log_softmax(next_token_logits, dim=-1)\n",
    "        \n",
    "        # 2. Expand: Calculate score for ALL possible next tokens for ALL beams\n",
    "        # If current beam score is X, new score is X + log_prob(token)\n",
    "        # We simulate this expansion and pick Top-K from the (beam_width * vocab) pool\n",
    "        \n",
    "        vocab_size = next_token_logits.shape[-1]\n",
    "        \n",
    "        # In a real loop, we add previous_scores + next_token_probs\n",
    "        # Here we just show the selection logic:\n",
    "        top_k_scores, top_k_indices = torch.topk(next_token_probs.view(-1), k=beam_width)\n",
    "        \n",
    "        # 3. Decode indices back to (Beam Index, Token Index)\n",
    "        beam_indices = top_k_indices // vocab_size\n",
    "        token_indices = top_k_indices % vocab_size\n",
    "        \n",
    "        # 4. Apply Length Penalty (Critical Exam Concept)\n",
    "        # Score = LogProb / (Length ^ alpha)\n",
    "        # This prevents the model from preferring extremely short sentences\n",
    "        current_length = input_ids.shape[1] + 1\n",
    "        penalty_factor = ((5 + current_length) / 6) ** length_penalty\n",
    "        adjusted_scores = top_k_scores / penalty_factor\n",
    "        \n",
    "        return beam_indices, token_indices, adjusted_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>2. The Enterprise Deployment (TensorRT-LLM)\n",
    "In the real exam and production, you do not write loops. You configure the Executor.\n",
    "\n",
    "This is how you enable Beam Search in a tensorrt_llm Python script.</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorrt_llm\n",
    "from tensorrt_llm.runtime import ModelRunner, SamplingConfig\n",
    "\n",
    "# 1. Initialize the Optimized Runtime\n",
    "runner = ModelRunner.from_dir(\"path/to/trt_engine_dir\")\n",
    "\n",
    "# 2. Define Inputs\n",
    "prompt = \"Describe the architecture of a Transformer.\"\n",
    "input_ids = tokenizer.encode(prompt)\n",
    "\n",
    "# --- SCENARIO A: GREEDY DECODING ---\n",
    "greedy_config = SamplingConfig(\n",
    "    end_id=tokenizer.eos_token_id,\n",
    "    pad_id=tokenizer.pad_token_id,\n",
    "    num_beams=1,      # Exam Key: num_beams=1 implies Greedy or Sampling (Top-K/P)\n",
    "    top_k=1,          # Strict Greedy\n",
    "    top_p=0.0\n",
    ")\n",
    "\n",
    "# --- SCENARIO B: BEAM SEARCH ---\n",
    "beam_config = SamplingConfig(\n",
    "    end_id=tokenizer.eos_token_id,\n",
    "    pad_id=tokenizer.pad_token_id,\n",
    "    num_beams=5,             # Exam Key: > 1 triggers Beam Search kernels\n",
    "    length_penalty=1.2,      # Exam Key: > 1.0 encourages longer output\n",
    "    early_stopping=True      # Stop when 5 full sentences are found\n",
    ")\n",
    "\n",
    "# 3. Run Inference\n",
    "# TRT-LLM handles the complex beam expansion/pruning in C++ kernels\n",
    "outputs = runner.generate(\n",
    "    batch_input_ids=[input_ids],\n",
    "    max_new_tokens=100,\n",
    "    sampling_config=beam_config \n",
    ")\n",
    "\n",
    "print(outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>3. Triton Inference Server Integration\n",
    "If you are using Triton (the preferred NVIDIA deployment method), you don't even write the Python code above. You pass parameters in the JSON Payload.</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text_input': 'Explain quantum computing',\n",
       " 'parameters': {'max_tokens': 128,\n",
       "  'beam_width': 5,\n",
       "  'length_penalty': 1.5,\n",
       "  'repetition_penalty': 1.0}}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "{\n",
    "  \"text_input\": \"Explain quantum computing\",\n",
    "  \"parameters\": {\n",
    "    \"max_tokens\": 128,\n",
    "    \"beam_width\": 5,          \n",
    "    \"length_penalty\": 1.5,    \n",
    "    \"repetition_penalty\": 1.0 \n",
    "  }\n",
    "}"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
