{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Attention Mechanisms</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Transformer attention : scaled dot-product</b>\n",
    "<p>\"Attention Is All You Need\" (Vaswani et al., Google Brain, 2017). This is the single most important paper in modern AI history.</p>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scaled Dot-Product Attention (Transformer Core)\n",
    "\n",
    "The core engine of the Transformer is **Scaled Dot-Product Attention**.\n",
    "\n",
    "**Analogy: Finding a book in a library**\n",
    "\n",
    "- **Query (Q):** What you are looking for  \n",
    "- **Key (K):** The label on the book spine  \n",
    "- **Value (V):** The content inside the book  \n",
    "\n",
    "The model computes a **matching score** between the **Query** and each **Key**.  \n",
    "Higher score ⇒ stronger alignment ⇒ the corresponding **Value** is retrieved (weighted more).\n",
    "\n",
    "## The Equation\n",
    "\n",
    "$$\n",
    "\\mathrm{Attention}(Q, K, V) = \\mathrm{softmax}\\left(\\frac{QK^{T}}{\\sqrt{d_k}}\\right)V\n",
    "$$\n",
    "\n",
    "### What each part means\n",
    "\n",
    "- **$QK^T$ (dot product):** Measures similarity.  \n",
    "  If **Query** and **Key** vectors are well-aligned, the dot product is high → **higher attention**.\n",
    "\n",
    "- **$\\sqrt{d_k}$ (scaling):** Prevents the dot-product values from becoming too large as the key dimension grows,  \n",
    "  which keeps **softmax** from saturating and helps training stay stable.\n",
    "\n",
    "- **$\\mathrm{softmax}$:** Converts the similarity scores into **probabilities** (weights that sum to 1).  \n",
    "  These weights decide how much each **Value** contributes to the final output.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Intuition: What Attention Really Does\n",
    "\n",
    "**Attention is a learned “focus” mechanism:** given a current need (a **Query**), it computes how relevant each piece of available information is (their **Keys**), then returns a **weighted mixture** of the corresponding **Values**.\n",
    "\n",
    "You can think of it as a **content-based lookup**:\n",
    "\n",
    "> “Given what I’m trying to produce right now, which parts of the input (or my own past tokens) should I look at, and how much?”\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Why Only the Last Two Dimensions?\n",
    "\n",
    "In deep learning, tensors are often **3D or 4D** to enable parallel processing. A typical Transformer tensor shape is:\n",
    "\n",
    "\\[\n",
    "[\\text{Batch\\_Size},\\ \\text{Num\\_Heads},\\ \\text{Seq\\_Length},\\ \\text{Head\\_Dim}]\n",
    "\\]\n",
    "\n",
    "### Batch_Size & Num_Heads → the “Wrapper”\n",
    "These dimensions **group independent computations** so the GPU can process many items at once.\n",
    "\n",
    "- **Batch_Size:** different training examples (must not mix)\n",
    "- **Num_Heads:** different attention heads (must not mix)\n",
    "\n",
    "We **never** want data from different batches or different heads to interact.\n",
    "\n",
    "### Seq_Length & Head_Dim → the “Data”\n",
    "These are the **actual matrices** involved in attention:\n",
    "\n",
    "- **Seq_Length:** how many tokens\n",
    "- **Head_Dim:** vector dimension per head\n",
    "\n",
    "### The Logic (Why `transpose(-2, -1)`)\n",
    "\n",
    "Matrix multiplication in PyTorch is **batch-aware**:\n",
    "\n",
    "- It performs matrix multiplication using the **last two dimensions**:\n",
    "  \\[\n",
    "  (M \\times N) \\cdot (N \\times P)\n",
    "  \\]\n",
    "- It treats all preceding dimensions as **independent batch dimensions**.\n",
    "\n",
    "So when you do:\n",
    "\n",
    "```python\n",
    "K.transpose(-2, -1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A. `key.transpose(-2, -1)`\n",
    "\n",
    "- **Action:** Swaps the **Sequence** dimension with the **Feature/Head-Dim** dimension.  \n",
    "- **Why:** To align dimensions correctly for the dot product.  \n",
    "- **Shape change:**\n",
    "  \\[\n",
    "  (\\ldots,\\ \\text{Seq},\\ \\text{Dim}) \\rightarrow (\\ldots,\\ \\text{Dim},\\ \\text{Seq})\n",
    "  \\]\n",
    "\n",
    "---\n",
    "\n",
    "### B. `torch.matmul(query, ...)`\n",
    "\n",
    "- **Action:** Computes dot products between **every Query vector** and **every Key vector**.  \n",
    "- **Math:**\n",
    "  \\[\n",
    "  QK^T\n",
    "  \\]\n",
    "- **Physical meaning:** This measures **similarity**. High values mean the Query is very similar to the Key (they “resonate”).  \n",
    "- **Result shape:**\n",
    "  \\[\n",
    "  (\\ldots,\\ \\text{Seq\\_Length},\\ \\text{Seq\\_Length})\n",
    "  \\]\n",
    "  This is a **square attention-score matrix**, showing the raw relationship score between every token and every other token.\n",
    "\n",
    "---\n",
    "\n",
    "### C. `* (1.0 / \\sqrt{d_k})` (Scaling Factor)\n",
    "\n",
    "- **Action:** Scales down raw scores by dividing by:\n",
    "  \\[\n",
    "  \\sqrt{d_k}\n",
    "  \\]\n",
    "  Example: if \\(d_k = 64\\), divide by \\(8\\).\n",
    "\n",
    "- **Why:** Prevents **softmax saturation** and supports stable gradients during backprop.\n",
    "\n",
    "#### The “Why” in detail\n",
    "\n",
    "1. **Exploding dot products:**  \n",
    "   When \\(d_k\\) is large, dot products can become large (e.g., 50 or 100).\n",
    "\n",
    "2. **Softmax saturation:**  \n",
    "   Feeding large values into softmax makes the largest score dominate:\n",
    "   - one probability ≈ 1.0  \n",
    "   - others ≈ 0.0\n",
    "\n",
    "3. **The consequence:**  \n",
    "   At extreme probabilities (near 0 or 1), softmax gradients become tiny → **gradients almost vanish**.\n",
    "\n",
    "4. **The fix:**  \n",
    "   Scaling brings scores into a friendlier range (often around \\(-2\\) to \\(+2\\)), so softmax remains sensitive and **gradients flow properly** during training.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def scaled_dot_product_attention(query, key, value, mask=None):\n",
    "    \"\"\"\n",
    "    query: (..., Tq, d_k)\n",
    "    key:   (..., Tk, d_k)\n",
    "    value: (..., Tk, d_v)\n",
    "    mask:  broadcastable to (..., Tq, Tk), with True=keep, False=mask-out\n",
    "    \"\"\"\n",
    "    d_k = query.size(-1)\n",
    "    print(d_k)\n",
    "\n",
    "    # scores: (..., Tq, Tk)\n",
    "    scores = torch.matmul(query, key.transpose(-2, -1)) * (1.0 / math.sqrt(d_k))\n",
    "\n",
    "    if mask is not None:\n",
    "        # mask should be bool with True = allowed positions\n",
    "        scores = scores.masked_fill(~mask, torch.finfo(scores.dtype).min)\n",
    "\n",
    "    attn = F.softmax(scores, dim=-1)\n",
    "    output = torch.matmul(attn, value)\n",
    "    return output, attn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ 6.2868e-01,  3.3294e-01, -1.0065e+00, -4.4684e-01, -1.3347e+00,\n",
      "          -2.2934e+00, -4.9829e-01,  1.1927e+00],\n",
      "         [-2.0786e-03,  1.8171e+00,  2.1088e-01,  1.9876e-02,  4.0393e-01,\n",
      "           1.4973e+00, -1.6074e-01, -4.1271e-01],\n",
      "         [-1.5201e+00, -2.5280e-01,  6.5119e-01,  2.5223e+00,  1.0555e+00,\n",
      "          -1.4796e+00, -1.8002e+00, -1.0275e+00],\n",
      "         [ 1.3638e+00, -6.9932e-01, -3.7744e-01, -1.0261e+00, -8.1618e-01,\n",
      "          -1.0265e+00,  2.4817e-01, -1.8578e-01]],\n",
      "\n",
      "        [[-1.5149e+00,  2.4313e-01, -2.0613e-01, -1.5240e+00, -4.1461e-01,\n",
      "           4.9132e-02, -1.1608e-02,  4.8200e-01],\n",
      "         [-1.2766e+00, -2.5119e+00,  9.4640e-01,  6.7175e-01, -2.8854e-01,\n",
      "           1.4067e+00, -1.1165e+00,  4.9560e-01],\n",
      "         [-4.3327e-02, -8.3183e-01, -1.3362e+00, -8.5593e-01,  1.6311e-01,\n",
      "          -2.2261e-01, -1.3148e+00, -2.0051e-01],\n",
      "         [ 8.3527e-01, -1.9039e+00, -2.2139e+00,  1.5670e+00,  5.3185e-02,\n",
      "           6.4601e-02,  9.5687e-01,  8.2319e-01]]])\n",
      "tensor([[[ 0.1911,  0.2146,  0.1906,  0.9015,  1.2503, -0.9860,  1.3091,\n",
      "          -0.9918],\n",
      "         [-0.5643,  1.1156,  1.0412,  0.1060, -0.8367, -0.9066,  0.2481,\n",
      "           0.5605],\n",
      "         [-1.0315,  0.1651, -0.3433,  1.2961, -0.8474,  0.3799,  1.3820,\n",
      "           0.8774],\n",
      "         [ 0.4879, -0.2580, -0.0640,  0.3088, -0.8182, -2.7187,  0.3783,\n",
      "          -0.8666]],\n",
      "\n",
      "        [[ 1.3059,  0.5188, -1.2561, -0.0188,  0.4883,  1.1617,  1.0410,\n",
      "          -0.7519],\n",
      "         [-0.8106, -0.0266,  0.2910,  1.2516,  0.6768,  0.3876, -0.3412,\n",
      "          -0.8467],\n",
      "         [-0.6621,  0.1657,  2.2011,  0.0217, -1.2468,  0.5715, -0.3950,\n",
      "           0.4716],\n",
      "         [-1.1778, -0.6486, -0.2750, -0.2340, -0.2053, -0.5115,  0.6110,\n",
      "          -0.2477]]])\n",
      "tensor([[[-1.2719, -0.9925, -1.5481,  1.3147,  0.5776,  1.8825,  0.4307,\n",
      "           2.2515],\n",
      "         [-1.7151,  0.4051, -0.0927,  0.3752,  0.3310, -0.5887, -0.5516,\n",
      "          -0.5425],\n",
      "         [-1.1526,  0.7242, -0.2984,  0.3144,  0.3665,  1.9686, -0.1769,\n",
      "           1.7342],\n",
      "         [ 0.0938,  0.2169,  0.0727,  0.0531,  0.8319,  1.6841,  0.3445,\n",
      "          -0.0192]],\n",
      "\n",
      "        [[ 0.2802, -2.2474,  0.5014, -1.9475, -0.5093,  1.7302,  0.3342,\n",
      "          -1.5037],\n",
      "         [ 1.1894,  0.6325,  0.2119, -0.2245,  0.3050, -0.2416,  0.6547,\n",
      "          -0.8831],\n",
      "         [ 0.3722,  0.0490, -1.1699,  1.9688,  0.4255, -0.8109,  0.3700,\n",
      "          -0.0855],\n",
      "         [-1.1562, -0.3395,  0.8017,  1.6927,  1.7590, -0.1877,  0.7444,\n",
      "           0.0346]]])\n",
      "8\n",
      "Self-attn out: torch.Size([2, 4, 8])\n",
      "Self-attn attn: torch.Size([2, 4, 4])\n"
     ]
    }
   ],
   "source": [
    "# -------------------------\n",
    "# 1) Self-attention example\n",
    "# -------------------------\n",
    "B, T, d = 2, 4, 8\n",
    "q = torch.randn(B, T, d)\n",
    "k = torch.randn(B, T, d)\n",
    "v = torch.randn(B, T, d)\n",
    "\n",
    "print(q)\n",
    "print(k)\n",
    "print(v)\n",
    "\n",
    "out, attn = scaled_dot_product_attention(q, k, v)\n",
    "print(\"Self-attn out:\", out.shape)     # (B, T, d)\n",
    "print(\"Self-attn attn:\", attn.shape)   # (B, T, T)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ 0.1911,  0.2146,  0.1906,  0.9015,  1.2503, -0.9860,  1.3091,\n",
      "          -0.9918],\n",
      "         [-0.5643,  1.1156,  1.0412,  0.1060, -0.8367, -0.9066,  0.2481,\n",
      "           0.5605],\n",
      "         [-1.0315,  0.1651, -0.3433,  1.2961, -0.8474,  0.3799,  1.3820,\n",
      "           0.8774],\n",
      "         [ 0.4879, -0.2580, -0.0640,  0.3088, -0.8182, -2.7187,  0.3783,\n",
      "          -0.8666]],\n",
      "\n",
      "        [[ 1.3059,  0.5188, -1.2561, -0.0188,  0.4883,  1.1617,  1.0410,\n",
      "          -0.7519],\n",
      "         [-0.8106, -0.0266,  0.2910,  1.2516,  0.6768,  0.3876, -0.3412,\n",
      "          -0.8467],\n",
      "         [-0.6621,  0.1657,  2.2011,  0.0217, -1.2468,  0.5715, -0.3950,\n",
      "           0.4716],\n",
      "         [-1.1778, -0.6486, -0.2750, -0.2340, -0.2053, -0.5115,  0.6110,\n",
      "          -0.2477]]])\n"
     ]
    }
   ],
   "source": [
    "print(k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ 0.1911, -0.5643, -1.0315,  0.4879],\n",
      "         [ 0.2146,  1.1156,  0.1651, -0.2580],\n",
      "         [ 0.1906,  1.0412, -0.3433, -0.0640],\n",
      "         [ 0.9015,  0.1060,  1.2961,  0.3088],\n",
      "         [ 1.2503, -0.8367, -0.8474, -0.8182],\n",
      "         [-0.9860, -0.9066,  0.3799, -2.7187],\n",
      "         [ 1.3091,  0.2481,  1.3820,  0.3783],\n",
      "         [-0.9918,  0.5605,  0.8774, -0.8666]],\n",
      "\n",
      "        [[ 1.3059, -0.8106, -0.6621, -1.1778],\n",
      "         [ 0.5188, -0.0266,  0.1657, -0.6486],\n",
      "         [-1.2561,  0.2910,  2.2011, -0.2750],\n",
      "         [-0.0188,  1.2516,  0.0217, -0.2340],\n",
      "         [ 0.4883,  0.6768, -1.2468, -0.2053],\n",
      "         [ 1.1617,  0.3876,  0.5715, -0.5115],\n",
      "         [ 1.0410, -0.3412, -0.3950,  0.6110],\n",
      "         [-0.7519, -0.8467,  0.4716, -0.2477]]])\n"
     ]
    }
   ],
   "source": [
    "print(k.transpose(-2, -1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Causal self-attn (decoder/GPT)</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8\n",
      "Causal out: torch.Size([2, 4, 8])\n",
      "Causal attn: torch.Size([2, 4, 4])\n"
     ]
    }
   ],
   "source": [
    "# -----------------------------------\n",
    "# 2) Causal self-attn (decoder/GPT)\n",
    "# -----------------------------------\n",
    "T = q.size(1)\n",
    "causal_mask = torch.tril(torch.ones(T, T, dtype=torch.bool))  # (T, T) broadcastable\n",
    "out_causal, attn_causal = scaled_dot_product_attention(q, k, v, mask=causal_mask)\n",
    "print(\"Causal out:\", out_causal.shape)        # (B, T, d)\n",
    "print(\"Causal attn:\", attn_causal.shape)      # (B, T, T)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Cross-attention example (enc-dec) - decoder queries attend over encoder keys/values</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8\n",
      "Cross-attn out: torch.Size([2, 3, 8])\n",
      "Cross-attn attn: torch.Size([2, 3, 5])\n"
     ]
    }
   ],
   "source": [
    "# -----------------------------------\n",
    "# 3) Cross-attention example (enc-dec)\n",
    "#    decoder queries attend over encoder keys/values\n",
    "# -----------------------------------\n",
    "B, T_dec, T_enc, d = 2, 3, 5, 8\n",
    "q_dec = torch.randn(B, T_dec, d)   # decoder states as queries\n",
    "k_enc = torch.randn(B, T_enc, d)   # encoder outputs as keys\n",
    "v_enc = torch.randn(B, T_enc, d)   # encoder outputs as values\n",
    "\n",
    "out_xattn, attn_xattn = scaled_dot_product_attention(q_dec, k_enc, v_enc)\n",
    "print(\"Cross-attn out:\", out_xattn.shape)     # (B, T_dec, d)\n",
    "print(\"Cross-attn attn:\", attn_xattn.shape)   # (B, T_dec, T_enc)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Padding mask example (variable-length sequences) - Suppose encoder has padding on the right.</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8\n",
      "Pad-masked cross-attn out: torch.Size([2, 2, 8])\n",
      "Pad-masked attn: torch.Size([2, 2, 5])\n"
     ]
    }
   ],
   "source": [
    "# -----------------------------------\n",
    "# 4) Padding mask example (variable-length sequences)\n",
    "#    Suppose encoder has padding on the right.\n",
    "# -----------------------------------\n",
    "lengths = torch.tensor([5, 3])  # batch: first has 5 valid, second has 3 valid\n",
    "B, T_enc, d = 2, 5, 8\n",
    "q_dec = torch.randn(B, 2, d)\n",
    "k_enc = torch.randn(B, T_enc, d)\n",
    "v_enc = torch.randn(B, T_enc, d)\n",
    "\n",
    "# mask shape (B, 1, T_enc) -> broadcast to (B, T_dec, T_enc)\n",
    "pad_mask = torch.arange(T_enc).unsqueeze(0) < lengths.unsqueeze(1)  # (B, T_enc) True=valid\n",
    "pad_mask = pad_mask.unsqueeze(1)  # (B, 1, T_enc)\n",
    "\n",
    "out_pad, attn_pad = scaled_dot_product_attention(q_dec, k_enc, v_enc, mask=pad_mask)\n",
    "print(\"Pad-masked cross-attn out:\", out_pad.shape)   # (B, T_dec, d)\n",
    "print(\"Pad-masked attn:\", attn_pad.shape)            # (B, T_dec, T_enc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
