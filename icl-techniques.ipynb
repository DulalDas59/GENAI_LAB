{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Zero-shot (instruction-only)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "import os\n",
    "\n",
    "client = OpenAI(base_url=os.environ[\"NIM_BASE_URL\"], api_key=os.getenv(\"NIM_API_KEY\",\"nvapi-...\"))\n",
    "\n",
    "def zero_shot(prompt: str, user_input: str):\n",
    "    return client.chat.completions.create(\n",
    "        model=os.environ[\"NIM_MODEL\"],\n",
    "        messages=[\n",
    "            {\"role\":\"system\",\"content\": prompt},\n",
    "            {\"role\":\"user\",\"content\": user_input},\n",
    "        ],\n",
    "        temperature=0.2,\n",
    "        top_p=0.9,\n",
    "        max_tokens=200\n",
    "    ).choices[0].message.content\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One-shot / Few-shot (demonstrations as message pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def few_shot(system_prompt: str, demos: list[tuple[str,str]], user_input: str):\n",
    "    messages = [{\"role\":\"system\",\"content\": system_prompt}]\n",
    "    for x, y in demos:\n",
    "        messages += [\n",
    "            {\"role\":\"user\",\"content\": x},\n",
    "            {\"role\":\"assistant\",\"content\": y},\n",
    "        ]\n",
    "    messages.append({\"role\":\"user\",\"content\": user_input})\n",
    "\n",
    "    return client.chat.completions.create(\n",
    "        model=os.environ[\"NIM_MODEL\"],\n",
    "        messages=messages,\n",
    "        temperature=0.2,\n",
    "        top_p=0.9,\n",
    "        max_tokens=200\n",
    "    ).choices[0].message.content\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example demos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt = \"Classify priority as P0/P1/P2/P3. Output only the label.\"\n",
    "\n",
    "demos = [\n",
    "  (\"Service down for all users. No workaround.\", \"P0\"),\n",
    "  (\"Intermittent errors for 10% users. Workaround exists.\", \"P1\"),\n",
    "  (\"Minor UI bug. No impact on core flow.\", \"P3\"),\n",
    "]\n",
    "\n",
    "print(few_shot(system_prompt, demos, \"Checkout failures in one region, workaround: manual retry.\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Production Implementation: Dynamic Prompt Selector\n",
    "Below is a Python conceptual implementation for an Enterprise Dynamic Few-Shot Selector. This is what you would build in a framework like LangChain or LlamaIndex."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "class DynamicFewShotSelector:\n",
    "    def __init__(self, example_library, embedding_model):\n",
    "        \"\"\"\n",
    "        example_library: List of dicts [{'input': '...', 'output': '...'}, ...]\n",
    "        embedding_model: Function that returns vector for text\n",
    "        \"\"\"\n",
    "        self.examples = example_library\n",
    "        self.embedding_model = embedding_model\n",
    "        # Pre-compute embeddings for the library (Cache this in production!)\n",
    "        self.library_vectors = [self.embedding_model(ex['input']) for ex in example_library]\n",
    "\n",
    "    def get_best_k_examples(self, user_query, k=3):\n",
    "        \"\"\"\n",
    "        Selects top k examples semantically similar to the user query.\n",
    "        \"\"\"\n",
    "        query_vector = self.embedding_model(user_query)\n",
    "        \n",
    "        # Calculate Cosine Similarity\n",
    "        scores = cosine_similarity([query_vector], self.library_vectors)[0]\n",
    "        \n",
    "        # Get indices of top k scores\n",
    "        top_indices = np.argsort(scores)[-k:][::-1]\n",
    "        \n",
    "        selected_examples = [self.examples[i] for i in top_indices]\n",
    "        return selected_examples\n",
    "\n",
    "    def construct_prompt(self, user_query, k=3):\n",
    "        # 1. Retrieve\n",
    "        shots = self.get_best_k_examples(user_query, k)\n",
    "        \n",
    "        # 2. Format (Enterprise Standard: ChatML or similar)\n",
    "        prompt_text = \"System: You are a helpful classifier.\\n\"\n",
    "        \n",
    "        # 3. Inject Few-Shot Examples\n",
    "        for shot in shots:\n",
    "            prompt_text += f\"User: {shot['input']}\\nAssistant: {shot['output']}\\n\"\n",
    "            \n",
    "        # 4. Append Actual Query\n",
    "        prompt_text += f\"User: {user_query}\\nAssistant:\"\n",
    "        \n",
    "        return prompt_text\n",
    "\n",
    "# Usage Scenario:\n",
    "# You have 1000 support tickets categorized.\n",
    "# User asks a new question.\n",
    "# We find the 3 most similar past tickets and feed them as few-shot examples."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
