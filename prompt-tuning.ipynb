{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is the Enterprise-Grade Implementation details for Prompt Tuning using the Hugging Face PEFT (Parameter-Efficient Fine-Tuning) library.\n",
    "\n",
    "This code demonstrates how to inject Soft Prompts into a frozen model. This is exactly what you would do in a production environment to adapt a generic LLM (like Llama 3 or Bloom) to a specific task (like Financial Sentiment Analysis) without the massive cost of full fine-tuning.\n",
    "\n",
    "Technical Implementation: Prompt Tuning via PEFT\n",
    "Prerequisites: pip install transformers peft torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from peft import get_peft_model, PromptTuningConfig, TaskType, PromptTuningInit\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# 1. Load the Base Model (Frozen)\n",
    "# In enterprise, this is your shared 7B or 70B model artifact.\n",
    "# ------------------------------------------------------------------\n",
    "model_name = \"bigscience/bloomz-560m\" # Using a small model for demonstration\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# 2. Define the Prompt Tuning Configuration\n",
    "# This is the critical architectural step.\n",
    "# ------------------------------------------------------------------\n",
    "peft_config = PromptTuningConfig(\n",
    "    task_type=TaskType.CAUSAL_LM,\n",
    "    \n",
    "    # PROMPT_TUNING_INIT: Crucial for Enterprise Stability\n",
    "    # initializing with 'RANDOM' is unstable. We initialize soft prompts \n",
    "    # using the embeddings of a concrete text instruction.\n",
    "    prompt_tuning_init=PromptTuningInit.TEXT,\n",
    "    \n",
    "    # The text to initialize the embeddings with:\n",
    "    prompt_tuning_init_text=\"Classify if the financial sentiment of this tweet is positive, neutral, or negative:\",\n",
    "    \n",
    "    # NUM_VIRTUAL_TOKENS: The length of the soft prompt.\n",
    "    # Research suggests 20-100 tokens is the sweet spot for most tasks.\n",
    "    # Note: The init text above is tokenized, and if shorter than this number,\n",
    "    # the remaining tokens are randomized.\n",
    "    num_virtual_tokens=20, \n",
    "    \n",
    "    tokenizer_name_or_path=model_name,\n",
    ")\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# 3. Inject the Soft Prompts (Wrap the Model)\n",
    "# ------------------------------------------------------------------\n",
    "model = get_peft_model(model, peft_config)\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# 4. Verify Parameter Efficiency (The \"Enterprise Value\" Check)\n",
    "# ------------------------------------------------------------------\n",
    "def print_trainable_parameters(model):\n",
    "    \"\"\"\n",
    "    Prints the number of trainable parameters in the model.\n",
    "    \"\"\"\n",
    "    trainable_params = 0\n",
    "    all_param = 0\n",
    "    for _, param in model.named_parameters():\n",
    "        all_param += param.numel()\n",
    "        if param.requires_grad:\n",
    "            trainable_params += param.numel()\n",
    "    \n",
    "    print(\n",
    "        f\"trainable params: {trainable_params} || \"\n",
    "        f\"all params: {all_param} || \"\n",
    "        f\"trainable%: {100 * trainable_params / all_param:.4f}\"\n",
    "    )\n",
    "\n",
    "print_trainable_parameters(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
