{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Enterprise-Grade Implementation: DPO with TRL\n",
    "To implement this, we use Hugging Face's TRL (Transformer Reinforcement Learning) library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from trl import DPOTrainer, DPOConfig\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from datasets import load_dataset\n",
    "\n",
    "# 1. Load Model and Reference\n",
    "# In DPO, usually model and ref are the same SFT checkpoint initially\n",
    "model_id = \"your-org/sft-llama-3-8b\"\n",
    "model = AutoModelForCausalLM.from_pretrained(model_id, device_map=\"auto\")\n",
    "ref_model = AutoModelForCausalLM.from_pretrained(model_id, device_map=\"auto\")\n",
    "\n",
    "# 2. Load Preference Data\n",
    "# Must have columns: 'prompt', 'chosen', 'rejected'\n",
    "dataset = load_dataset(\"json\", data_files=\"internal_prefs.json\")\n",
    "\n",
    "# 3. Define DPO Configuration\n",
    "dpo_args = DPOConfig(\n",
    "    beta=0.1,                 # The \"Temperature\" of DPO. 0.1 is standard. \n",
    "                              # Higher beta = more deviation from ref allowed.\n",
    "    learning_rate=5e-7,       # VERY small LR is crucial for DPO\n",
    "    per_device_train_batch_size=2,\n",
    "    gradient_accumulation_steps=4,\n",
    "    output_dir=\"./dpo-aligned-model\",\n",
    ")\n",
    "\n",
    "# 4. Initialize Trainer\n",
    "trainer = DPOTrainer(\n",
    "    model=model,\n",
    "    ref_model=ref_model,\n",
    "    args=dpo_args,\n",
    "    train_dataset=dataset,\n",
    "    tokenizer=tokenizer,\n",
    ")\n",
    "\n",
    "# 5. Train\n",
    "trainer.train()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
